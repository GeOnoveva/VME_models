---
title: "Whole workflow"
output: html_notebook
---

# Intro


# Libraries
```{r}
#install_github("trinker/qdapTools")
library(BAMMtools)
library(caret)
library(DescTools)
library(doParallel)
library(dplyr)
library(groupdata2)
library(qdapTools)
library(quantregForest)
library(ranger)
library(raster)
library(readxl)
library(rgdal)
library(RPostgreSQL)
library(spm)
library(tidyverse)
library(yardstick)
library(parallel)
library(doParallel)
```

# Load data
```{r}
# species densities (marvid data)
# for now use "fake marvid report" script

#data <- read.csv(file.path(Data, "species_abundance.csv"), sep = "|")
#data <- data %>% makesampid

# Taxonary
taxonary <- read_xlsx(file.path(Data,"Taxonary.xlsx"), sheet = 1) # needs to be closed
```

## Lump detailed identifications with their corresponding taxon, and filter out generic/broad taxa
Only applicable when using data directly downloaded from Marvid
```{r}
# lump

# data <- data %>% mutate(taxon=lookup(taxon, data.frame(select(taxonary, c(Reference_List, Analysis_name))), missing=NULL)) %>%
#   group_by(taxon, SampID) %>%
#   summarize(density = sum(abundance.Nper100m.2)) %>%
#   select(SampID, taxon, density)
# 
# filter out generic taxa

head(data)
```

# Derive VME data
Use 2023 NovasArc Report

## available VME indicators
```{r}
print(unique(taxonary$VME_BuhlMortensen_etal_2023))
```
## Do they have the same environmental niche, approximately?
Or are they within the same biotope, at least? Need to answer yes to one of these two questions before going ahead. If answer is no, then cut indicators out. This is done in a separate notebook. Return here after you have run it
```{r}
# set vme
vme <- "Hard bottom sponge aggregations (1)"
remove <- "Axinellidae"
indicators <- taxonary$Reference_List[which(taxonary$VME_BuhlMortensen_etal_2023==vme)]
#indicators <- indicators[-which(indicators%in%remove)]
indicators
```

## Filter data by VME indicators
```{r}
#derive response
resp1 <- data %>% group_by(SampID) %>%
  summarize(tot_rich = length(SampID), tot_dens = sum(density))
  
  
resp2 <- data %>% filter(taxon%in%indicators) %>%
  group_by(SampID) %>%
  summarize(vmeind_density = sum(density))

resp <- left_join(resp1,resp2) %>% mutate(vmeind_density=replace_na(vmeind_density,0))
resp
```

## Explore resp
```{r}
hist(resp$vmeind_density)
```



# Save output
```{r}
rm(resp1, resp2)
write.csv(resp, file.path(savepath, paste(vme, "minus", paste(substr(remove, 1,7), collapse=", "), "resp.csv", collapse=" ")))
```

## Get sample info
### If Marvid runing use this:
```{python}
import requests

API_URL = "http://marvid-staging.hi.no:8092/query/video_lines"
OUTPUL_FILEPATH = "C:\\Users\\genoveva\\Downloads\\marvid_sections.csv"
DELIMITER = ","
CSV_HEADER = DELIMITER.join(["video line", "section", "mean latitude", "mean longitude"])

api_response = requests.get(API_URL).json() 
output_strings = [CSV_HEADER] 
for vl_number in sorted(api_response.keys()):
    vl = api_response[vl_number]
    for section in sorted(vl["sections"]):
        mean_lat = str(vl["sections"][section]["mean_lat"])
        mean_lon = str(vl["sections"][section]["mean_lon"])
        output_strings.append(DELIMITER.join([vl_number, section, mean_lat, mean_lon]))

with open(OUTPUL_FILEPATH, "w") as output_file:
    output_file.write("\n".join(output_strings))n
```

### If not, use this:
```{r}
sample_info <- read.csv(file.path(Data, "sample_info.csv"))
sample_info <- sample_info %>% rename(video.line=VL, section=SampID2, mean.latitude=y_coord, mean.longitude=x_coord) %>%
  dplyr::select(video.line, section, mean.latitude, mean.longitude, SampID)
# sample_info <- sample_info %>% makesampid # only needed when we switch to using marvid sections
```

# Aggregate to whole video line and calculate sd
```{r}
respw <- resp %>% mutate(video.line=sub("_.*", "", SampID))

respw <- respw %>% group_by(video.line) %>%
  summarize(mean_rich = mean(tot_rich), mean_dens = mean(vmeind_density), sd_dens = sd(vmeind_density)) %>%
  mutate(video.line = as.integer(video.line))

head(respw)
```

# Make spatial object
```{r}
resp_spat1 <- resp %>% mutate(mean_long=as.numeric(pull(
                                dplyr::filter(data.frame(lookup(SampID,dplyr::select(sample_info, c(SampID, mean.longitude)), missing = NULL)), 
                                                      !duplicated(data.frame(qdapTools::lookup(SampID,dplyr::select(sample_info, c(SampID, mean.longitude)), missing = NULL)))
                                                       ))),
                             mean_lat=as.numeric(pull(
                               dplyr::filter(data.frame(lookup(SampID,dplyr::select(sample_info, c(SampID, mean.latitude)), missing = NULL)), 
                                                      !duplicated(data.frame(qdapTools::lookup(SampID,dplyr::select(sample_info, c(SampID, mean.latitude)), missing = NULL)))
                                                       )))) %>%
            filter(!is.na(mean_long))

resp_spat <- SpatialPointsDataFrame(dplyr::select(resp_spat1, c(mean_long, mean_lat)), dplyr::select(resp_spat1, c(SampID,tot_rich, tot_dens, vmeind_density)))  
                      #%>% utmize

writeOGR(resp_spat, savepath, paste(vme, "minus", paste(substr(remove, 1,7), collapse=", "), "resp_spat"), driver = "ESRI Shapefile", overwrite_layer = TRUE)

# make resp have the same number of rows as resp_spat:

resp <- resp %>% filter(SampID%in%resp_spat@data$SampID)
head(resp)
```
# Get coordinates of video lines
```{r}
library(RPostgreSQL)
library(rgdal)

postgres_driver <- dbDriver("PostgreSQL")
postgres_conn <- dbConnect(postgres_driver,
                           dbname = "marbunn",
                           host = "postgres.hi.no",
                           port = 5432,
                           user = "marbunn_read",
                           password = "Ecac7adgb!")
tab_station <- dbGetQuery(postgres_conn, "select * from tab_station")
tab_station <- tab_station[which(tab_station$equipment==2),]

resp_spat1 <- respw %>% mutate(mean_long=as.numeric(pull(
  dplyr::filter(data.frame(lookup(video.line,dplyr::select(tab_station, c(sample_no, lon_mid_dec)), missing = NULL))))),
  mean_lat=as.numeric(pull(
    dplyr::filter(data.frame(lookup(video.line,dplyr::select(tab_station, c(sample_no, lat_mid_dec)), missing = NULL)))))) %>%
  dplyr::filter(!is.na(mean_long)) %>% rename(vmeind_density = mean_dens)

coords <- coordinates(dplyr::select(resp_spat1, c(mean_long, mean_lat)))

resp_spat_vl <- SpatialPointsDataFrame(coords, dplyr::select(resp_spat1, c(video.line,mean_rich, vmeind_density, sd_dens))) %>%
                       utmize

#writeOGR(resp_spat_vl, savepath, paste(vme, "minus", paste(substr(remove, 1,7), collapse=", "),  "resp_spat_vl"), driver = "ESRI Shapefile", overwrite_layer = TRUE)

# make resp have the same number of rows as resp_spat:

respw <- respw %>% filter(video.line%in%resp_spat_vl@data$video.line)

```

# Save outputs
```{r}
rm(resp_spat1)
write.csv(resp, file.path(savepath, paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "),"resp new.csv", collapse=" ")))
write.csv(respw, file.path(savepath, paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "), "st_dev_vme_dens.csv")), row.names = FALSE)
#saveRDS(resp_spat_vl, file.path(savepath, paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "), "resp_Spat_vl.rds")))
saveRDS(resp_spat, file.path(savepath, paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "), "resp_spat.rds")))
```

# Inputs
```{r}
e <- read.csv(file.path(savepath,"pred_data/e.csv"))
stdev <- respw
```

# join predictor values, response values, and standard deviation values
```{r}
e <- cbind(e,sample_info)
v <- left_join(resp,data.frame(e), by="SampID")
dim(v)
```

# join standard deviation of vme density
```{r}
v <- v %>% left_join(stdev, by="video.line")
```

# save
```{r}
write.csv(dplyr::select(v,
                        SampID,
                        tot_rich,
                        tot_dens,
                        vmeind_density,
                        mean_dens,
                        mean_rich,
                        sd_dens,
                        bathy,
                        BO22_carbonphytoltmax_bdmean,
                        BO22_carbonphytoltmax_ss,
                        BO22_carbonphytoltmin_bdmean,
                        BO22_carbonphytoltmin_ss,
                        BO22_carbonphytomean_bdmean,
                        BO22_carbonphytomean_ss,
                        BO22_carbonphytorange_bdmean,
                        BO22_carbonphytorange_ss,    
                        BO22_chloltmin_bdmean,    
                        BO22_chlomean_ss,    
                        BO22_dissoxltmax_bdmean,    
                        BO22_dissoxrange_bdmean,    
                        BO22_icecovermean_ss,    
                        BO22_icethickltmin_ss,    
                        BO22_ironltmax_bdmean,    
                        BO22_ironrange_bdmean,    
                        BO22_lightbotmean_bdmean,    
                        BO22_nitrateltmin_bdmean,    
                        BO22_phosphateltmax_bdmean,    
                        BO22_phosphaterange_bdmean,    
                        BO22_ppltmin_bdmean,    
                        BO22_ppmean_ss,    
                        BO22_silicateltmax_bdmean,    
                        BO22_silicaterange_bdmean,    
                        BO22_chloltmax_bdmean,
                        BO22_chloltmax_ss,
                        BO22_chloltmin_ss,
                        BO22_chlomean_bdmean,
                        BO22_chlorange_bdmean,
                        BO22_chlorange_ss,
                        BO22_dissoxltmin_bdmean,
                        BO22_dissoxmean_bdmean,
                        BO22_icecoverltmax_ss,
                        BO22_icecoverltmin_ss,
                        BO22_icecoverrange_ss,
                        BO22_icethickltmax_ss,
                        BO22_icethickmean_ss,
                        BO22_icethickrange_ss,
                        BO22_ironltmin_bdmean,
                        BO22_ironmean_bdmean,
                        BO22_lightbotltmax_bdmean,
                        BO22_lightbotltmin_bdmean,
                        BO22_lightbotrange_bdmean,
                        BO22_nitrateltmax_bdmean,
                        BO22_nitratemean_bdmean,
                        BO22_nitraterange_bdmean,
                        BO22_phosphateltmin_bdmean,
                        BO22_phosphatemean_bdmean,
                        BO22_ppltmax_bdmean,
                        BO22_ppltmax_ss,
                        BO22_ppltmin_ss,
                        BO22_ppmean_bdmean,
                        BO22_pprange_bdmean,
                        BO22_pprange_ss,
                        BO22_silicateltmin_bdmean,
                        BO22_silicatemean_bdmean,
                        CDirmax_Robinson,
                        CDirmean_Robinson,
                        CDirmin_Robinson,    
                        CDirsd_Robinson,
                        CSpdmax_Robinson,    
                        CSpdmean_Robinson,
                        CSpdmin_Robinson,
                        CSpdsd_Robinson,    
                        MLDmax_Robinson,
                        MLDmean_Robinson,    
                        MLDmin_Robinson,
                        MLDsd_Robinson,
                        MS_biogeo05_dist_shore_5m,
                        Smax_Robinson,    
                        Smean_Robinson,
                        Smin_Robinson,
                        Ssd_Robinson,
                        Tmax_Robinson,                  
                        Tmean_Robinson,
                        Tmin_Robinson,
                        Tsd_Robinson,                  
                        Umax_Robinson,
                        Umean_Robinson,
                        Umin_Robinson,                  
                        Usd_Robinson,
                        Vmax_Robinson,                  
                        Vmean_Robinson,
                        Vmin_Robinson,
                        Vsd_Robinson,
                        cobB,
                        diffME3,
                        diffME9,
                        gravel,    
                        landscape,
                        msr1_mag,    
                        msr5_mag,
                        mud,    
                        rock,
                        salt_max,
                        salt_mean,    
                        salt_min,
                        salt_std,
                        sand,    
                        sedclass,
                        seddan,
                        sedmil,    
                        slope3,
                        slope9,
                        spd_max,                  
                        spd_mean,
                        spd_min,
                        spd_std,                  
                        temp_max,
                        temp_mean,                  
                        temp_min,
                        temp_std,
                        u_bott_mean,
                        v_bott_mean		    
			   ), file.path(savepath, paste(vme, "minus", paste(substr(remove, 1,7), collapse=", "), "v.csv", collapse=" ")), row.names = FALSE)
```


```{r}
v <- v %>% 
      mutate_at(vars(  landscape, sedclass, seddan, sedmil    ), factor)

v <- v %>% filter(complete.cases(.))
dim(v)
```
# More inputs: pred
```{r}
#load(file.path(savepath,"1_Predictors_out.RData"))
pred.pix <- readRDS(file.path(savepath,"pred_data/pred.rds"))
pred.pix1 <- subset(pred.pix, complete.cases(pred.pix@data))
pred.pix1@data <- pred.pix1@data %>% mutate_at(vars(  landscape, sedclass, seddan, sedmil    ), factor)
```

# select environmental variables
```{r}
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
a=which(colnames(v)=="bathy")
b=which(colnames(v)=="v_bott_mean")
c=which(colnames(v)=="vmeind_density")
results <- rfe(v[,a:b], v[,c], sizes=c(1:(b-a)), rfeControl=control)
# chosen features
selvar <- predictors(results)
selvar
```

# add spatial variables
```{r}
# resp.dist0 <-buffer.dist(resp_spat_vl["vmeind_density"], 
#                                  pred.pix[1], as.factor(1:nrow(resp_spat_vl))) 
# 
# ov.resp = over(resp_spat["vmeind_density"], resp.dist0)
# 
# ov.resp <- over(resp_spat["vmeind_density"], pred.pix[1:2])
# sw.rm = do.call(cbind, list(resp_spat@data["vmeind_density"], ov.resp
#                             , ov.resp
#                             ))
# 
# v <- v %>% left_join(sw.rm) %>% filter(complete.cases(.))
```

# Set model formula
```{r}
all_names <- paste(#c(names(resp.dist0),
                     selvar, collapse="+")
fmla0 <- as.formula(paste("vmeind_density ~ ", 
                           all_names))
fmla0
```

# fine tune mtry parameter
```{r}
# configure multicore
#registerDoMC(cores=12)
#cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)

control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(1)
tunegrid <- expand.grid(.mtry=c(1:length(selvar)))
rf_gridsearch <- train(fmla0, data=v, method="qrf", metric="Rsquared", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
mtry=rf_gridsearch$bestTune
```

# train new model
without case weights
```{r}
m1 <- ranger(formula = fmla0,
             data = v,
             mtry=pull(mtry), 
             min.node.size=2,
             sample.fraction=0.9930754, 
             num.trees=250,
             importance = "impurity",
             seed=1,
             quantreg=TRUE,
             respect.unordered.factors = "partition"
             )
m1
```

with case weights
```{r}
m2 <- ranger(formula = fmla0,
             data = v,
             mtry=pull(mtry), 
             min.node.size=2,
             sample.fraction=0.9930754, 
             num.trees=250,
             importance = "impurity",
             seed=1,
             quantreg=TRUE,
             respect.unordered.factors = "partition",
             case.weights = loc_hom(v$sd_dens)
             )
m2
```

# Split data for model testing
split the data into two partitions where (1) video lines are kept together and (2) the testing data is biased towards intermediate to high-density observations
```{r}
p=0.2 # proportion of the core dataset will be used for testing the model (fringe dataset will be 50/50)

split <- stratisplit2(x=v, p=p)

v_test <- split[[1]]
v_train <- split[[2]]

rbind(hist(log(v_train$vmeind_density)), hist(log(v_test$vmeind_density)))

```

# test model
```{r}
m3 <- ranger(formula = fmla0,
             data = v_train,
             mtry=pull(mtry), 
             min.node.size=2,
             sample.fraction=0.9930754, 
             num.trees=250,
             importance = "impurity",
             seed=1,
             quantreg=TRUE,
             respect.unordered.factors = "partition"#,
             #case.weights = as.integer(1/(v$sd_dens^2))
             )
m3
```

# variance explained by cross-validation (from spm library)
```{r}
vme.qrf2 <- predict(m3,  v_test, 
                     type="quantiles", quantiles=quantiles)$predictions
## now more computational...
v_test$vme.qrf2pr = vme.qrf2[,2]
## s.d. of the prediction error:
v_test$vme.qrf2var = (vme.qrf2[,3]-vme.qrf2[,1])/2

predicted <- v_test$vme.qrf2pr
observed <- v_test %>%
  dplyr::select(vmeind_density)

vecv <- vecv(pull(observed),pull(data.frame(unlist(predicted)))) # variance explained by cross-validation, conservative estimate to report
vecv
```

# concordance correlation coefficient
```{r}
ccc <- ccc_vec(pull(observed),predicted)
ccc
```
# Save outputs
```{r}

saveRDS(m3, file = file.path(outpath,paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "), "model.rda" )))
write.csv(data.frame(SampID=c(as.character(v_test$SampID), as.character(v_train$SampID)), set=c(rep("test",length(v_test$SampID)),
                                                                                                rep("train", length(v_train$SampID)))),
          file.path(outpath, paste(vme,"minus", paste(substr(remove, 1,7), collapse=", "), "data split.csv")),
          row.names = FALSE)
```


